---
title: "Build up a targets pipeline"
output: html_notebook
---

# About

To implement the customer churn prediction analysis, we will create a `targets` pipeline. A pipeline is a high-level collection of steps, or *targets*, that comprise the workflow. Each *target* has a *command*, which is just a piece of R code that returns a value. In practice, targets are usually data cleaning, model fitting, and results summary steps, and the commands are just the R function calls that do those tasks.

# Functions

The commands of your targets will depend on the functions from `1-functions.Rmd`. For the exercises below, they are in `2-pipelines/functions.R`. Take a quick glance at `2-pipelines/functions.R` and refamiliarize yourself with the code base. 

# Start with a small pipeline.

The first version of the pipeline consists of three *targets*, or steps of computation. Our first targets do the following:

1. Reproducibly track the customer churn CSV data file.
2. Read in the data from the CSV file.
3. Preprocess the data for the deep learning models.

The steps above translate to the following R code.

```{r, eval = FALSE}
library(keras)
library(recipes)
library(rsample)
library(tidyverse)
library(yardstick)
source("2-pipelines/functions.R")
churn_file <- "data/churn.csv"             # Sketch of target 1.
churn_data <- split_data(churn_file)       # Sketch of target 2.
churn_recipe <- prepare_recipe(churn_data) # Sketch of target 3.
```

To formalize our three targets, we express the computation in a special configuration file called `_targets.R` at the project's root directory. The role of `_targets.R` is to

1. Load any functions and global objects that the targets depend on.
2. Set any options, including packages that the targets depend on.
3. Declare the targets and pipeline using functions `tar_target()` and `tar_pipeline()`. The `_targets.R` script must always end with a pipeline object, preferably an explicit call to `tar_pipeline()`.

The `tar_script()` function is a convenient way to write `_targets.R`.

```{r}
library(targets)
tar_script({
  source("2-pipelines/functions.R")
  tar_option_set(packages = c("keras", "recipes", "rsample", "tidyverse", "yardstick"))
  tar_pipeline(
    tar_target(churn_file, "data/churn.csv", format = "file"),
    tar_target(churn_data, split_data(churn_file)),
    tar_target(churn_recipe, prepare_recipe(churn_data))
  )
})
```

It is standard practice to make further modifications to `_targets.R` by hand, but for pedagogical simplicity, we will call `tar_script()` often. Let's inspect `_targets.R`.

```{r}
cat(readLines("_targets.R"), sep = "\n")
```

User-side functions in `targets` rely heavily on the `_targets.R` file. Examples are  `tar_validate()`, `tar_manifest()`, `tar_glimpse()`, and `tar_visnetwork()`. Each of these functions creates a fresh, clean, reproducible R process that runs `_targets.R` and then inspects the pipeline in some way. It is good practice to frequently run these functions to check your work.

```{r}
tar_validate() # Looks for errors.
```

```{r}
tar_manifest(fields = "command") # Data frame of target info.
```

```{r}
tar_glimpse() # Interactive dependency graph.
```

# Dependency relationships

`tar_glimpse()` is particularly helpful because it shows how the targets depend on one another. As the arrows indicate, target `churn_recipe` depends on `churn_data`, and target `churn_data` depends on `churn_file`. The `targets` package detects these relationships automatically using static code analysis. `churn_data` depends on `churn_file` because the command for `churn_data` mentions the symbol `churn_file`. You can see these dependency relationships for yourself using `codetools::findGlobals()`.

```{r}
codetools::findGlobals(function() split_data(churn_file))
```

That means the order you write your targets does not matter. Even if you rearrange the calls to `tar_target()` inside `tar_pipeline()`, you will still get the same workflow.

`tar_visnetwork()` also includes functions in the dependency graph, as well as color-coded status information.

```{r}
tar_visnetwork()
```

# Run the pipeline.

Everything we did so far was just setup. To actually run the pipeline, use the `tar_make()` function. `tar_make()` creates a fresh new reproducible R process that runs `_targets.R` and executes the correct targets in the correct order (from the dependency graph).

```{r}
tar_make()
```

# Inspect the results

Targets `churn_data` and `churn_recipe` now live in a special `_targets/` data store.

```{r}
list.files("_targets/objects")
```
`churn_file` is an [external input file](https://wlandau.github.io/targets-manual/files.html#external-input-files), declared with `format = "file"` in `tar_target()`, so its value is not in the datastore. However, the actual file path, hash, and other metadata are stored in the `_targets/meta/meta` spreadsheet.

```{r}
cat(readLines("_targets/meta/meta"), sep = "\n")
```

But rather than access the files in `_targets/` directly, it is good practice to go through user-side functions in `targets`.

```{r}
tar_load(churn_file)
churn_file
```

```{r}
tar_read(churn_data)
```


```{r}
tar_read(churn_recipe)
```

```{r}
meta <- tar_meta(starts_with("churn_"), fields = c("data", "format", "path"))
meta
```

```{r}
as.character(meta$path)
```

# Build up the pipeline.

After inspecting our current targets with `tar_load()` and `tar_read()`, we are ready to add new targets to fit our models. Informally, the new computations look like this:

```{r, eval = FALSE}
run_relu <- test_model(act1 = "relu", churn_data, churn_recipe)       # Model 1
run_sigmoid <- test_model(act1 = "sigmoid", churn_data, churn_recipe) # Model 2
```

Your turn: add these new steps to the pipeline as formal targets.

```{r}
library(targets)
tar_script({
  source("2-pipelines/functions.R")
  tar_option_set(packages = c("keras", "recipes", "rsample", "tidyverse", "yardstick"))
  tar_pipeline(
    tar_target(churn_file, "data/churn.csv", format = "file"),
    tar_target(churn_data, split_data(churn_file)),
    tar_target(churn_recipe, prepare_recipe(churn_data)),
    tar_target("???", "???"), # Your turn: add model 1.
    tar_target("???", "???")  # Your turn: add model 2.
  )
})
```











```{r}
plan <- drake_plan(
  churn_data = split_data(file_in("../data/customer_churn.csv")),
  churn_recipe = prepare_recipe(churn_data),
  run_relu = ,  # YOUR TURN: write the command between the equals sign and comma.
  run_sigmoid = # YOUR TURN: write the command here.
)
```

Visualize the graph to check the plan. `run_relu` and `run_sigmoid` should be new targets that depend on `churn_data`, `churn_recipe`, `test_model()`, and the custom functions called from `test_model()`.

```{r}
vis_drake_graph(plan)
```

`churn_data` and `churn_recipe` are up to date from last time, and `run_relu` and `run_sigmoid` are new.

```{r}
outdated(plan)
```

So `make()` skips the data and just runs the models. Run `make()` below and ignore any TensorFlow messages that pop up.

```{r}
make(plan) # slow
```

Those models took a long time to run, right? That is why `make()` skips them if they are up to date.

```{r}
outdated(plan)
```

```{r}
make(plan) # fast
```

Now is a good time to check your model runs. Each should be a data frame with the accuracy and features of a model.

```{r, paged.print = FALSE}
readd(run_relu)
```

```{r, paged.print = FALSE}
readd(run_sigmoid)
```

# Add another model

Now, let's add a third model with a different activation function. Use the `act1 = "softmax"` this time.

```{r}
plan <- drake_plan(
  churn_data = split_data(file_in("../data/customer_churn.csv")),
  churn_recipe = prepare_recipe(churn_data),
  run_relu = test_model(act1 = "relu", churn_data, churn_recipe),
  run_sigmoid = test_model(act1 = "sigmoid", churn_data, churn_recipe),
  run_softmax = # YOUR TURN: write a new call to test_model() with act1 = "softmax"
)
```

The previous two runs should be up to date. Only `run_softmax` should be outdated.

```{r}
outdated(plan)
```

```{r}
vis_drake_graph(plan)
```

Run the softmax model.

```{r}
make(plan)
```

Did it come out right?

```{r, paged.print = FALSE}
readd(run_softmax)
```

# Pick the best model.

Define a new `best_run` target with following command:

```{r, eval = FALSE}
# Do not run here.
bind_rows(run_relu, run_sigmoid, run_softmax) %>%
  top_n(1, accuracy) %>%
  head(1)
```

Write the command in the plan below. Note: commands do not always need to be strict function calls. They can be arbitrary code chunks too, such as the one above.

```{r}
plan <- drake_plan(
  churn_data = split_data(file_in("../data/customer_churn.csv")),
  churn_recipe = prepare_recipe(churn_data),
  run_relu = test_model(act1 = "relu", churn_data, churn_recipe),
  run_sigmoid = test_model(act1 = "sigmoid", churn_data, churn_recipe),
  run_softmax = test_model(act1 = "softmax", churn_data, churn_recipe),
  best_run = # YOUR TURN: write the command to get the best model run.
)
```

The next `make()` should just build `best_run` and be quick.

```{r}
make(plan)
```

`best_run` should contain one row with the accuracy and features of the best model.

```{r}
readd(best_run)
```

Now, let's train the best model and hold on to the trained model object. Below, we use `drake`'s `target()` function for extra customization because Keras models need the special `"keras"` storage format (<https://books.ropensci.org/drake/plans.html#special-data-formats-for-targets>). Below, write the command `train_best_model(best_run, churn_recipe)`.

```{r}
plan <- drake_plan(
  churn_data = split_data(file_in("../data/customer_churn.csv")),
  churn_recipe = prepare_recipe(churn_data),
  run_relu = test_model(act1 = "relu", churn_data, churn_recipe),
  run_sigmoid = test_model(act1 = "sigmoid", churn_data, churn_recipe),
  run_softmax = test_model(act1 = "softmax", churn_data, churn_recipe),
  best_run = bind_rows(run_relu, run_sigmoid, run_softmax) %>%
    top_n(1, accuracy) %>%
    head(1),
  best_model = target( # We use target() to customize the target.
    , # YOUR TURN: write train_best_model(best_run, churn_recipe) to the left of the comma.
    format = "keras" # Tells drake to store the target as a Keras model.
  )
)
```

Train the model.

```{r}
make(plan)
```

The actual trained model is now in `drake`'s cache.

```{r}
readd(best_model)
```

# Changes

What if a function changes?

```{r}
define_model <- function(churn_recipe, units1, units2, act1, act2, act3) {
  input_shape <- ncol(
    juice(churn_recipe, all_predictors(), composition = "matrix")
  )
  keras_model_sequential() %>%
    layer_dense(
      units = units1,
      kernel_initializer = "uniform",
      activation = act1,
      input_shape = input_shape
    ) %>%
    layer_dropout(rate = 0.2) %>% # previously 0.1
    layer_dense(
      units = units2,
      kernel_initializer = "uniform",
      activation = act2
    ) %>%
    layer_dropout(rate = 0.1) %>%
    layer_dense(
      units = 1,
      kernel_initializer = "uniform",
      activation = act3
    )
}
```

`drake` automatically detects the change and rebuilds the affected targets.

```{r}
outdated(plan)
```

```{r}
vis_drake_graph(plan)
```

```{r}
make(plan)
```

What if we undo the change?

```{r}
define_model <- function(churn_recipe, units1, units2, act1, act2, act3) {
  input_shape <- ncol(
    juice(churn_recipe, all_predictors(), composition = "matrix")
  )
  keras_model_sequential() %>%
    layer_dense(
      units = units1,
      kernel_initializer = "uniform",
      activation = act1,
      input_shape = input_shape
    ) %>%
    layer_dropout(rate = 0.1) %>% # Changed back to 0.1.
    layer_dense(
      units = units2,
      kernel_initializer = "uniform",
      activation = act2
    ) %>%
    layer_dropout(rate = 0.1) %>%
    layer_dense(
      units = 1,
      kernel_initializer = "uniform",
      activation = act3
    )
}
```

You can tell `drake` to recover old targets instead of building new ones from scratch.

```{r}
make(plan, recover = TRUE)
```

Same story if you change a `file_in()` dataset (or change it back).

# Time savings and reproducibility

All your targets should be up to date now.

```{r}
outdated(plan)
```

```{r}
make(plan)
```

This is tangible evidence of reproducibility. It is tangible evidence that your output is in sync with the code and data you have right now. So if someone else were to run your code, they should get the same results as you. You can be confident without having to rerun everything from scratch. This is how `drake` saves time while increasing your ability to trust your research.

# Recap

Building up a plan is an gradual, iterative process:

1. Add or change some targets targets in the plan.
2. Check the plan with `outdated(plan)` and `vis_drake_graph(plan)`.
3. Run `make()` to build the new targets.
4. Check the output with `loadd()` and `readd()`.
5. Repeat.
